# -*- coding: utf-8 -*-
"""UserBehaviourclass.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10f1XI68lvmyPq_JufOMXQ_rVIPHtNb6Y
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

# Load dataset
df = pd.read_csv('/content/user_behavior_dataset.csv')
df.head()

# Check for missing values
print(df.isnull().sum())

# Encoding categorical features
categorical_features = ['Device Model', 'Operating System', 'Gender']
encoder = LabelEncoder()
for feature in categorical_features:
    df[feature] = encoder.fit_transform(df[feature])

# Scaling numerical features
scaler = StandardScaler()
numerical_features = ['Age', 'Screen On Time (hours/day)', 'Data Usage (MB/day)', 'Battery Drain (mAh/day)']
df[numerical_features] = scaler.fit_transform(df[numerical_features])

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize feature distributions
for feature in numerical_features:
    plt.figure()
    sns.histplot(df[feature], kde=True)
    plt.title(f'Distribution of {feature}')
    plt.show()

# Analyze target variable (User Behavior Class) for imbalance
sns.countplot(x='User Behavior Class', data=df)
plt.title('Class Distribution')

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Feature Correlation')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Split data
X = df.drop('User Behavior Class', axis=1)
y = df['User Behavior Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train models
models = {
    'Logistic Regression': LogisticRegression(solver='saga', max_iter=300),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier()
}

# Evaluate each model
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"Model: {model_name}")
    print(classification_report(y_test, y_pred))

from sklearn.metrics import classification_report, accuracy_score

# Evaluate each model
for model_name, model in models.items():
    y_pred = model.predict(X_test)
    print(f"\nModel: {model_name}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV

# Define parameter grid for tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20],
    'class_weight': ['balanced', 'balanced_subsample']  # Adjust weights for imbalance
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='f1')
grid_search.fit(X_res, y_res)

# Best model and parameters
best_model = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# Evaluate the tuned model on the test set
y_pred_tuned = best_model.predict(X_test)
print("Tuned Model Performance on Test Set:")
print("Accuracy:", accuracy_score(y_test, y_pred_tuned))
print(classification_report(y_test, y_pred_tuned))

from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation with F1 scoring for multiclass problems
cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='f1_weighted')  # or use 'f1_macro' or 'f1_micro'
print("Cross-Validation F1 Scores:", cv_scores)
print("Mean CV F1 Score:", cv_scores.mean())

# Feature importance for best model
importances = best_model.feature_importances_
feature_names = X.columns
sorted_indices = np.argsort(importances)[::-1]

print("Top Influential Features:")
for idx in sorted_indices[:5]:  # top 5 features
    print(f"{feature_names[idx]}: {importances[idx]}")

! pip install streamlit

# Save the model
import joblib
joblib.dump(best_model, 'best_model.pkl')

# Streamlit app
import streamlit as st
import joblib
import numpy as np

# Load the model
model = joblib.load('best_model.pkl')

import warnings
warnings.filterwarnings("ignore", message="missing ScriptRunContext")

# Title and input form
st.title("User Behavior Classification")
age = st.number_input("Age")
screen_time = st.number_input("Screen Time")
data_usage = st.number_input("Data Usage")
battery_drain = st.number_input("Battery Drain")

# Predict button
if st.button("Predict"):
    features = np.array([[age, screen_time, data_usage, battery_drain]])
    prediction = model.predict(features)
    st.write("Predicted User Behavior Class:", prediction[0])

# Run locally
# streamlit run <script_name.py>

